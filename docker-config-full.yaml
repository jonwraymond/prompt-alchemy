# Full Configuration for Prompt Alchemy with All Providers
# Uses environment variables from .env file

# Provider configurations
providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "${OPENAI_MODEL:-o4-mini}"
    timeout: 30
  
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    model: "${ANTHROPIC_MODEL:-claude-4-sonnet-20250522}"
    timeout: 30
  
  google:
    api_key: "${GOOGLE_API_KEY}"
    model: "${GOOGLE_MODEL:-gemini-2.5-flash}"
    timeout: 60

  openrouter:
    api_key: "${OPENROUTER_API_KEY}"
    model: "anthropic/claude-3.5-sonnet"
    timeout: 30
    
  ollama:
    base_url: "http://ollama:11434"
    model: "llama3.2:3b"
    timeout: 120

# Phase configurations (distributed across providers)
phases:
  prima-materia:
    provider: "openai"
  solutio:
    provider: "anthropic"  
  coagulatio:
    provider: "google"

# Embedding configuration
embeddings:
  standard_model: "text-embedding-3-small"
  standard_dimensions: 1536
  provider_priority:
    - "openai"
    - "anthropic"
    - "google"
    - "ollama"
  auto_migrate_legacy: true
  cache_embeddings: true
  similarity_threshold: 0.3

# Generation settings
generation:
  default_temperature: 0.7
  default_max_tokens: 2000
  default_count: 3
  use_parallel: true

# Data storage
data_dir: "/app/data"

# Logging
log_level: "info"

# Learning settings
learning:
  enabled: true
  learning_rate: 0.1
  decay_rate: 0.01
  min_confidence: 0.6
  feedback_window: 24h

# Ranking configuration
ranking:
  weights:
    temperature: 0.2
    token: 0.2
    semantic: 0.3
    length: 0.1
    historical: 0.2

# Client mode configuration
client:
  mode: "local"                              # "local" or "client"
  server_url: "http://localhost:9090"        # Server URL for client mode  
  timeout: 30                                # Client timeout in seconds