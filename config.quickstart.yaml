# Prompt Alchemy Configuration
# This configuration is designed for quickstart/MCP usage

# Generation settings
generation:
  default_temperature: 0.7
  default_max_tokens: 2000
  default_count: 3
  use_parallel: true
  default_target_model: "claude-4-sonnet-20250522"
  default_embedding_model: "text-embedding-3-small"
  default_embedding_dimensions: 1536

# Phase configuration - using available providers
phases:
  prima-materia:
    provider: "openai"
  solutio:
    provider: "openai" 
  coagulatio:
    provider: "openai"

# Provider configuration (API keys from environment)
providers:
  openai:
    model: "gpt-4"
    embedding_model: "text-embedding-3-small"
  anthropic:
    model: "claude-3-sonnet-20240229"
  google:
    model: "gemini-1.5-pro-latest"
  grok:
    model: "grok-beta"
  openrouter:
    model: "auto"

# Judge configuration
judge:
  provider: "openai"
  model: "gpt-4"

# Storage settings
storage:
  type: "hybrid"  # Uses SQLite + chromem-go
  path: "/app/data"

# Learning settings
learning:
  enabled: true
  min_feedback_score: 0.7
  update_interval: 100

# Self-learning settings
self_learning:
  enabled: true                    # Use historical data to enhance prompts
  min_relevance_score: 0.7        # Minimum score for historical prompts
  max_examples: 3                 # Maximum examples to include in context
  use_patterns: true              # Extract and use patterns from history
  use_insights: true              # Include analytical insights

# Embedding settings for self-learning
embeddings:
  provider: "openai"              # Provider for generating embeddings
  model: "text-embedding-3-small" # Embedding model
  dimensions: 1536                # Embedding dimensions

# Server settings
server:
  host: "0.0.0.0"
  port: 8080
  
# MCP settings
mcp:
  log_level: "info"