# Example Prompt Alchemy Configuration
# Copy this to ~/.prompt-alchemy/config.yaml and add your API keys

# Provider configurations
providers:
  openai:
    api_key: "sk-your-openai-api-key-here"
    model: "o4-mini"
    timeout: 30
  
  openrouter:
    api_key: "sk-or-your-openrouter-api-key-here" 
    model: "openrouter/auto"  # Auto-select best available model
    base_url: "https://openrouter.ai/api/v1"
    timeout: 30
    # Fallback models (optional) - includes latest Claude 4
    fallback_models:
      - "anthropic/claude-sonnet-4"
      - "anthropic/claude-3.5-sonnet"
      - "openai/o4-mini"
    # Provider routing for enhanced privacy/features (optional)
    provider_routing:
      require_parameters: false
      data_collection: "deny"
      allow_fallbacks: true
  
  claude:
    api_key: "sk-ant-your-anthropic-api-key-here"
    model: "claude-sonnet-4-20250514"  # Latest Claude 4 Sonnet
    timeout: 30
  
  gemini:
    api_key: "your-google-api-key-here"
    model: "gemini-2.5-flash"
    timeout: 60  # HTTP timeout in seconds
    safety_threshold: "BLOCK_MEDIUM_AND_ABOVE"  # Safety filter threshold
    max_pro_tokens: 1024   # Max tokens for Pro models
    max_flash_tokens: 512  # Max tokens for Flash models
    default_tokens: 256    # Default token limit
    max_temperature: 2.0   # Maximum temperature allowed

  ollama:
    base_url: "http://localhost:11434"
    model: "gemma3:4b"
    timeout: 60  # HTTP timeout in seconds
    default_embedding_model: "nomic-embed-text"  # Default embedding model
    embedding_timeout: 5     # Embedding timeout in seconds
    generation_timeout: 120  # Generation timeout in seconds

  grok:
    api_key: "your-grok-api-key-here"
    model: "grok-2-1212"
    timeout: 30

# Phase configurations - mix and match providers
phases:
  prima-materia:
    provider: "openai"        # Use ChatGPT for raw essence extraction
  solutio:
    provider: "claude"        # Use Claude for natural language flow
  coagulatio:
    provider: "gemini"        # Use Gemini for precision crystallization

# Embedding configuration - STANDARDIZED for optimal search coverage
embeddings:
  # Standard embedding model for all prompts (ensures dimension compatibility)
  standard_model: "text-embedding-3-small"  # 1536 dimensions - optimal for semantic search
  standard_dimensions: 1536
  
  # Provider preference order for embeddings
  provider_priority:
    - "openai"      # Primary: text-embedding-3-small
    - "anthropic"   # Fallback: will use text-embedding-3-small
    - "google"      # Fallback: will use text-embedding-3-small equivalent
  
  # Migration settings
  auto_migrate_legacy: true    # Automatically re-embed prompts with non-standard dimensions
  migration_batch_size: 10     # Process embeddings in batches during migration
  
  # Performance settings
  cache_embeddings: true       # Cache embeddings to avoid re-computation
  similarity_threshold: 0.3    # Default minimum similarity for semantic search

# Generation settings
generation:
  default_temperature: 0.7    # Creativity level (0-1)
  default_max_tokens: 2000    # Maximum response length
  default_count: 3            # Number of variants per phase
  use_parallel: true          # Generate variants in parallel

# Data storage location (defaults to ~/.prompt-alchemy)
data_dir: "~/.prompt-alchemy"

# Logging level (debug, info, warn, error)
log_level: "info" 