# Docker Configuration for Prompt Alchemy with Ollama
# Configuration for testing without API keys

# Provider configurations (using Ollama only)
providers:
  ollama:
    base_url: "http://ollama:11434"
    model: "llama3.2:3b"
    timeout: 120

# Phase configurations (all using Ollama)
phases:
  prima-materia:
    provider: "ollama"
  solutio:
    provider: "ollama"
  coagulatio:
    provider: "ollama"

# Embedding configuration (disabled for Ollama testing)
embeddings:
  provider_priority:
    - "ollama"
  auto_migrate_legacy: false
  cache_embeddings: true
  similarity_threshold: 0.3

# Generation settings
generation:
  default_temperature: 0.7
  default_max_tokens: 2000
  default_count: 3
  use_parallel: true

# Data storage (container path)
data_dir: "/app/data"

# Logging
log_level: "debug"

# Learning settings (disabled for testing)
learning:
  enabled: false

# Ranking configuration
ranking:
  weights:
    temperature: 0.2
    token: 0.2
    semantic: 0.3
    length: 0.1
    historical: 0.2